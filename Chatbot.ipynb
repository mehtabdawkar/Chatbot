{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYBtk_HWrvvk"
      },
      "source": [
        "## 1) Conversation Manager with Summarization\n",
        "\n",
        "This section implements:\n",
        "- Keeping a running conversation history (list of dicts: `{role: 'user'|'assistant', 'content': ...}`)\n",
        "- Truncation options:\n",
        "  - By number of turns (last `n` messages)\n",
        "  - By character or word length (approximate)\n",
        "- Periodic summarization after every `k` runs: compress the earlier messages into a summary message and replace them in history.\n",
        "\n",
        "A simple summarizer is provided (extractive + heuristic). Replace with Groq/OpenAI calls for better results."
      ],
      "id": "XYBtk_HWrvvk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5spb3u2Trvvl"
      },
      "source": [
        "from typing import List, Dict, Optional\n",
        "import textwrap\n",
        "import re\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "\n",
        "class ConversationManager:\n",
        "    def __init__(self):\n",
        "        # history: list of messages: each is {'role': 'user'|'assistant'|'system', 'content': str, 'ts': str}\n",
        "        self.history: List[Dict] = []\n",
        "        self.run_count = 0\n",
        "\n",
        "    def add_message(self, role: str, content: str):\n",
        "        self.history.append({\n",
        "            'role': role,\n",
        "            'content': content,\n",
        "            'ts': datetime.utcnow().isoformat() + 'Z'\n",
        "        })\n",
        "\n",
        "    def get_history(self):\n",
        "        return list(self.history)\n",
        "\n",
        "    def truncate_by_turns(self, last_n: int) -> List[Dict]:\n",
        "        if last_n <= 0:\n",
        "            return []\n",
        "        return self.history[-last_n:]\n",
        "\n",
        "    def truncate_by_chars(self, max_chars: int) -> List[Dict]:\n",
        "        # keep as many *most recent* messages as fit under max_chars (approx)\n",
        "        out = []\n",
        "        total = 0\n",
        "        for m in reversed(self.history):\n",
        "            c = len(m['content'])\n",
        "            if total + c > max_chars and out:\n",
        "                break\n",
        "            out.append(m)\n",
        "            total += c\n",
        "        return list(reversed(out))\n",
        "\n",
        "    def truncate_by_words(self, max_words: int) -> List[Dict]:\n",
        "        out = []\n",
        "        total = 0\n",
        "        for m in reversed(self.history):\n",
        "            w = len(re.findall(r\"\\\\w+\", m['content']))\n",
        "            if total + w > max_words and out:\n",
        "                break\n",
        "            out.append(m)\n",
        "            total += w\n",
        "        return list(reversed(out))\n",
        "\n",
        "    def simple_summarize(self, messages: List[Dict], max_sentences: int = 3) -> str:\n",
        "        \"\"\"\n",
        "        A heuristic summarizer: take first and last sentences + a few highest-length sentences.\n",
        "        Replace this with a proper GPT/Groq call for production quality.\n",
        "        \"\"\"\n",
        "        text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
        "        # split into sentences roughly\n",
        "        sents = re.split(r'(?<=[.!?])\\\\s+', text)\n",
        "        sents = [s.strip() for s in sents if s.strip()]\n",
        "        if not sents:\n",
        "            return ''\n",
        "        picks = []\n",
        "        # first sentence\n",
        "        picks.append(sents[0])\n",
        "        # last sentence\n",
        "        if len(sents) > 1:\n",
        "            picks.append(sents[-1])\n",
        "        # then the longest sentences\n",
        "        middle = sorted(sents[1:-1], key=lambda x: -len(x))[: max(0, max_sentences - len(picks))]\n",
        "        picks.extend(middle)\n",
        "        summary = ' '.join(picks)\n",
        "        # clamp length\n",
        "        return textwrap.shorten(summary, width=800, placeholder='...')\n",
        "\n",
        "    def periodic_summarize(self, k: int, max_sentences: int = 3, replace: bool = True):\n",
        "        \"\"\"\n",
        "        Perform summarization every k runs. When called, it increments run_count. If run_count is divisible by k,\n",
        "        it summarizes everything except the last turn and replaces older messages with a single `assistant` summary message.\n",
        "        \"\"\"\n",
        "        self.run_count += 1\n",
        "        if k <= 0:\n",
        "            return None\n",
        "        if self.run_count % k != 0:\n",
        "            return None\n",
        "        # Summarize all but the last N messages (we'll keep last turn intact). You can customize this.\n",
        "        if len(self.history) <= 1:\n",
        "            return None\n",
        "        # we'll keep last 2 messages (recent context) and summarize the rest\n",
        "        keep_recent = 2\n",
        "        to_summarize = self.history[:-keep_recent]\n",
        "        if not to_summarize:\n",
        "            return None\n",
        "        summary_text = self.simple_summarize(to_summarize, max_sentences=max_sentences)\n",
        "        summary_message = {'role': 'assistant', 'content': f\"[Summary]: {summary_text}\", 'ts': datetime.utcnow().isoformat() + 'Z'}\n",
        "        if replace:\n",
        "            self.history = [summary_message] + self.history[-keep_recent:]\n",
        "        else:\n",
        "            self.history = [summary_message] + self.history\n",
        "        return summary_text\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5spb3u2Trvvl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqjha4qrvvm"
      },
      "source": [
        "### Demo: feed sample conversations and show truncation + periodic summarization\n"
      ],
      "id": "ARqjha4qrvvm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKAVu2hPrvvn",
        "outputId": "5c0cd57d-ad1b-4ca8-e1ae-b64a45212fa7"
      },
      "source": [
        "# Prepare demo conversation samples\n",
        "cm = ConversationManager()\n",
        "\n",
        "sample_convos = [\n",
        "    (\"user\", \"Hi, I need help planning a trip to Kyoto next month. I'm thinking 5 days.\"),\n",
        "    (\"assistant\", \"Great! What are your dates and interests (temples, food, hiking)?\"),\n",
        "    (\"user\", \"Around October 10-15. Mostly temples and food, budget-friendly.\"),\n",
        "    (\"assistant\", \"I recommend visiting Fushimi Inari early morning, Arashiyama on a day trip, and trying kaiseki. Do you want hotel recommendations?\"),\n",
        "    (\"user\", \"Yes please. Also, can you suggest transport from Osaka?\"),\n",
        "    (\"assistant\", \"Take the Haruka limited express from Osaka to Kyoto for a balance of speed and cost. For hotels, do you prefer traditional (ryokan) or modern hotels?\"),\n",
        "    (\"user\", \"Modern hotels but with local flavor. Also, I have dietary restrictions: no shellfish.\"),\n",
        "    (\"assistant\", \"Noted — I'll filter restaurants. Anything else?\"),\n",
        "    (\"user\", \"That's it for now, thank you.\")\n",
        "]\n",
        "\n",
        "for role, text in sample_convos:\n",
        "    cm.add_message(role, text)\n",
        "\n",
        "print('--- Full history ---')\n",
        "for m in cm.get_history():\n",
        "    print(f\"{m['role']}: {m['content']}\")\n",
        "\n",
        "print('\\n--- Truncate by last 4 turns ---')\n",
        "for m in cm.truncate_by_turns(4):\n",
        "    print(f\"{m['role']}: {m['content']}\")\n",
        "\n",
        "print('\\n--- Truncate by max 200 chars (approx) ---')\n",
        "for m in cm.truncate_by_chars(200):\n",
        "    print(f\"{m['role']}: {m['content']}\")\n",
        "\n",
        "print('\\n--- Periodic summarization demo (k=3): calling 3 times to trigger) ---')\n",
        "for i in range(3):\n",
        "    res = cm.periodic_summarize(k=3, max_sentences=3)\n",
        "    print(f'Run {i+1}, summary produced: ', bool(res))\n",
        "    if res:\n",
        "        print('\\nSummary text:\\n', res)\n",
        "        print('\\nHistory after replace:\\n')\n",
        "        for m in cm.get_history():\n",
        "            print(f\"{m['role']}: {m['content']}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Full history ---\n",
            "user: Hi, I need help planning a trip to Kyoto next month. I'm thinking 5 days.\n",
            "assistant: Great! What are your dates and interests (temples, food, hiking)?\n",
            "user: Around October 10-15. Mostly temples and food, budget-friendly.\n",
            "assistant: I recommend visiting Fushimi Inari early morning, Arashiyama on a day trip, and trying kaiseki. Do you want hotel recommendations?\n",
            "user: Yes please. Also, can you suggest transport from Osaka?\n",
            "assistant: Take the Haruka limited express from Osaka to Kyoto for a balance of speed and cost. For hotels, do you prefer traditional (ryokan) or modern hotels?\n",
            "user: Modern hotels but with local flavor. Also, I have dietary restrictions: no shellfish.\n",
            "assistant: Noted — I'll filter restaurants. Anything else?\n",
            "user: That's it for now, thank you.\n",
            "\n",
            "--- Truncate by last 4 turns ---\n",
            "assistant: Take the Haruka limited express from Osaka to Kyoto for a balance of speed and cost. For hotels, do you prefer traditional (ryokan) or modern hotels?\n",
            "user: Modern hotels but with local flavor. Also, I have dietary restrictions: no shellfish.\n",
            "assistant: Noted — I'll filter restaurants. Anything else?\n",
            "user: That's it for now, thank you.\n",
            "\n",
            "--- Truncate by max 200 chars (approx) ---\n",
            "user: Modern hotels but with local flavor. Also, I have dietary restrictions: no shellfish.\n",
            "assistant: Noted — I'll filter restaurants. Anything else?\n",
            "user: That's it for now, thank you.\n",
            "\n",
            "--- Periodic summarization demo (k=3): calling 3 times to trigger) ---\n",
            "Run 1, summary produced:  False\n",
            "Run 2, summary produced:  False\n",
            "Run 3, summary produced:  True\n",
            "\n",
            "Summary text:\n",
            " user: Hi, I need help planning a trip to Kyoto next month. I'm thinking 5 days. assistant: Great! What are your dates and interests (temples, food, hiking)? user: Around October 10-15. Mostly temples and food, budget-friendly. assistant: I recommend visiting Fushimi Inari early morning, Arashiyama on a day trip, and trying kaiseki. Do you want hotel recommendations? user: Yes please. Also, can you suggest transport from Osaka? assistant: Take the Haruka limited express from Osaka to Kyoto for a balance of speed and cost. For hotels, do you prefer traditional (ryokan) or modern hotels? user: Modern hotels but with local flavor. Also, I have dietary restrictions: no shellfish.\n",
            "\n",
            "History after replace:\n",
            "\n",
            "assistant: [Summary]: user: Hi, I need help planning a trip to Kyoto next month. I'm thinking 5 days. assistant: Great! What are your dates and interests (temples, food, hiking)? user: Around October 10-15. Mostly temples and food, budget-friendly. assistant: I recommend visiting Fushimi Inari early morning, Arashiyama on a day trip, and trying kaiseki. Do you want hotel recommendations? user: Yes please. Also, can you suggest transport from Osaka? assistant: Take the Haruka limited express from Osaka to Kyoto for a balance of speed and cost. For hotels, do you prefer traditional (ryokan) or modern hotels? user: Modern hotels but with local flavor. Also, I have dietary restrictions: no shellfish.\n",
            "assistant: Noted — I'll filter restaurants. Anything else?\n",
            "user: That's it for now, thank you.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3746006406.py:17: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  'ts': datetime.utcnow().isoformat() + 'Z'\n",
            "/tmp/ipython-input-3746006406.py:94: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  summary_message = {'role': 'assistant', 'content': f\"[Summary]: {summary_text}\", 'ts': datetime.utcnow().isoformat() + 'Z'}\n"
          ]
        }
      ],
      "id": "ZKAVu2hPrvvn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4vMTwQ9rvvn"
      },
      "source": [
        "## 2) JSON Schema Classification & Information Extraction\n",
        "\n",
        "We'll create a JSON schema and implement a function-calling like interface that would be compatible with Groq/OpenAI's `function_call` outputs.\n",
        "Because this environment may not have network access, we **simulate** the assistant's structured response. Replace the simulated part with real API calls to Groq/OpenAI client (example code included) when running in Colab with your keys.\n"
      ],
      "id": "A4vMTwQ9rvvn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSjZyfixrvvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c8627e-9198-4f20-d66c-b7970f8ed30f"
      },
      "source": [
        "import json\n",
        "from typing import Any, Dict, Tuple\n",
        "\n",
        "# Define the JSON schema we want to extract to (simple version)\n",
        "SCHEMA = {\n",
        "    'name': {'type': 'string', 'required': False},\n",
        "    'email': {'type': 'string', 'required': False, 'pattern': r'^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$'},\n",
        "    'phone': {'type': 'string', 'required': False, 'pattern': r'\\\\+?[0-9\\\\-\\\\s]{7,20}$'},\n",
        "    'location': {'type': 'string', 'required': False},\n",
        "    'age': {'type': 'integer', 'required': False, 'min': 0, 'max': 130}\n",
        "}\n",
        "\n",
        "def simple_validate(instance: Dict[str, Any], schema: Dict[str, Any]) -> Tuple[bool, Dict[str,str]]:\n",
        "    \"\"\"Validate a dict against the simple SCHEMA above. Returns (ok, errors).\n",
        "    This is a small custom validator to avoid external dependencies. For production, use `jsonschema`.\n",
        "    \"\"\"\n",
        "    errors = {}\n",
        "    ok = True\n",
        "    for k, rules in schema.items():\n",
        "        val = instance.get(k)\n",
        "        if val is None:\n",
        "            if rules.get('required'):\n",
        "                errors[k] = 'missing required'\n",
        "                ok = False\n",
        "            continue\n",
        "        t = rules.get('type')\n",
        "        if t == 'string':\n",
        "            if not isinstance(val, str):\n",
        "                errors[k] = 'expected string'\n",
        "                ok = False\n",
        "            pat = rules.get('pattern')\n",
        "            if pat and isinstance(val, str):\n",
        "                if not re.match(pat, val):\n",
        "                    errors[k] = 'pattern mismatch'\n",
        "                    ok = False\n",
        "        elif t == 'integer':\n",
        "            if not isinstance(val, int):\n",
        "                # try to coerce\n",
        "                try:\n",
        "                    ival = int(val)\n",
        "                    instance[k] = ival\n",
        "                except Exception:\n",
        "                    errors[k] = 'expected integer'\n",
        "                    ok = False\n",
        "                    continue\n",
        "            if 'min' in rules and instance[k] < rules['min']:\n",
        "                errors[k] = 'too small'\n",
        "                ok = False\n",
        "            if 'max' in rules and instance[k] > rules['max']:\n",
        "                errors[k] = 'too large'\n",
        "                ok = False\n",
        "    return ok, errors\n",
        "\n",
        "def simulate_function_call_parse(chat_text: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    A simulated function-calling extractor that returns structured JSON likely to be\n",
        "    produced by an OpenAI/Groq function-calling response. Replace with live API call.\n",
        "    \"\"\"\n",
        "    # naive extraction heuristics\n",
        "    out = {}\n",
        "    # name: look for 'my name is' or 'this is'\n",
        "    m = re.search(r\"(?:my name is|i am|this is)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\", chat_text, re.I)\n",
        "    if m:\n",
        "        out['name'] = m.group(1).strip()\n",
        "    # email\n",
        "    m = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.[a-zA-Z]{2,}\", chat_text)\n",
        "    if m:\n",
        "        out['email'] = m.group(0)\n",
        "    # phone\n",
        "    m = re.search(r\"(\\\\+?[0-9][0-9\\-\\s]{6,19}[0-9])\", chat_text)\n",
        "    if m:\n",
        "        out['phone'] = re.sub(r\"[^0-9+]\", '', m.group(0))\n",
        "    # location: look for 'from <Place>' or 'in <Place>'\n",
        "    m = re.search(r\"(?:from|in)\\s+([A-Z][a-zA-Z\\s,]+)\", chat_text)\n",
        "    if m:\n",
        "        out['location'] = m.group(1).strip()\n",
        "    # age\n",
        "    m = re.search(r\"\\b(\\d{1,3})\\s*(?:years old|yo|yrs old|y/o)\\b\", chat_text, re.I)\n",
        "    if m:\n",
        "        try:\n",
        "            out['age'] = int(m.group(1))\n",
        "        except:\n",
        "            pass\n",
        "    return out\n",
        "\n",
        "# Example live API call (commented) - replace with your Groq/OpenAI-compatible client code\n",
        "\n",
        "client = Groq(\n",
        "    api_key=os.environ.get(\"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\"),\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"openai/gpt-oss-20b\",\n",
        "    stream=False,\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Fast language models**—those that can generate text or respond to prompts with low latency and high throughput—are becoming the linchpin of practical AI today.  While research papers often celebrate raw accuracy (e.g., perplexity or BLEU scores), the real‑world value of a language model lies in how quickly it can deliver that value to users, businesses, and devices.\n",
            "\n",
            "Below is a concise but deep dive into why speed matters, how it impacts different stakeholders, and the technical tricks that make it possible.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Why Speed Matters\n",
            "\n",
            "| Domain | Why Speed is Crucial | Real‑World Consequences of Slowness |\n",
            "|--------|---------------------|-------------------------------------|\n",
            "| **Consumer Apps (chatbots, voice assistants, on‑device typing aids)** | Users expect near‑instant responses. Even a 200 ms delay feels laggy. | Lost user engagement, higher churn, negative brand perception. |\n",
            "| **Enterprise Search & Recommendation** | Millions of queries per second; each query must be answered within a few hundred ms to keep search rankings relevant. | Reduced productivity, stale recommendations, missed revenue. |\n",
            "| **Gaming & Virtual Worlds** | Real‑time dialogue or procedural content generation. | Bot lag breaks immersion, can crash games. |\n",
            "| **Edge/IoT Devices** | Limited compute, battery, and bandwidth. | Models cannot be run locally if inference takes seconds. |\n",
            "| **Large‑Scale Cloud Services** | Cost is tied to compute time; higher latency means higher charges per inference. | Budget blowouts, need for more hardware, higher carbon footprint. |\n",
            "\n",
            "### Key Takeaway\n",
            "\n",
            "*Speed directly translates to **user experience**, **operational cost**, **scalability**, and **environmental impact**.*\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Speed vs Accuracy: The Classic Trade‑off\n",
            "\n",
            "| Aspect | Fast Models | High‑Accuracy Models |\n",
            "|--------|-------------|----------------------|\n",
            "| **Inference Latency** | < 10 ms (single‑token) | 100–500 ms per token |\n",
            "| **Throughput** | > 10 k QPS on a single GPU | ~1–2 k QPS |\n",
            "| **Compute Cost** | Lower | Higher |\n",
            "| **Energy Use** | Lower | Higher |\n",
            "| **Use Cases** | Real‑time chat, voice dictation, on‑device typing | Research, long‑form generation, policy‑sensitive analysis |\n",
            "\n",
            "Most production systems adopt **efficient architectures** that provide “good enough” performance at a fraction of the cost.  In many contexts, a 1–2% loss in perplexity is acceptable if the model becomes 10× faster.\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Techniques to Make Models Fast\n",
            "\n",
            "| Technique | What It Does | Typical Impact |\n",
            "|-----------|--------------|----------------|\n",
            "| **Quantization** | Replace 32‑bit floating point weights with 8‑bit integers. | 3–4× speedup, 75–80 % memory reduction. |\n",
            "| **Pruning** | Remove redundant weights or entire attention heads. | 2–5× speedup, minimal accuracy loss. |\n",
            "| **Knowledge Distillation** | Train a smaller “student” network to mimic a large “teacher”. | 5–10× speedup, 10–20 % smaller model. |\n",
            "| **Sparse Attention** | Use block‑sparse or local attention patterns. | 2–3× speedup on long‑context tasks. |\n",
            "| **Model Parallelism & Pipeline Parallelism** | Split the model across GPUs. | Enables larger models to stay fast on many GPUs. |\n",
            "| **Streaming / Chunked Inference** | Generate tokens incrementally rather than waiting for entire context. | Lower perceived latency, supports real‑time UI updates. |\n",
            "| **Hardware‑specific Optimizations** | Use TensorRT, ONNX Runtime, or vendor‑optimized kernels. | 1.5–3× speedup over generic frameworks. |\n",
            "| **Auto‑ML for Architecture Search** | Find the minimal architecture that satisfies a latency budget. | Custom models tailored to deployment hardware. |\n",
            "\n",
            "> **Bottom line:** The fastest models are usually *not* the largest—they’re the most **efficient**.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Speed in the Cloud vs Edge\n",
            "\n",
            "| Scenario | Bottlenecks | How Speed Helps |\n",
            "|----------|-------------|-----------------|\n",
            "| **Cloud‑based inference** | Bandwidth, queueing, per‑second cost | Faster queries reduce queue time, lower compute cost per token. |\n",
            "| **Edge devices** | CPU/GPU limits, battery, latency to cloud | Local fast inference eliminates round‑trip latency and saves data usage. |\n",
            "| **Hybrid** | Need to offload when complexity spikes | Fast local model can handle most queries, fallback to cloud only when needed. |\n",
            "\n",
            "> **Example:** An 8‑bit quantized GPT‑2‑small runs in ~30 ms on a Snapdragon 8 Gen 1 phone, enabling on‑device writing assistance.\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Speed as a Democratizing Force\n",
            "\n",
            "* **Open‑source & low‑barrier deployments:** Fast, lightweight models let hobbyists run AI on a laptop or even a Raspberry Pi.\n",
            "* **Accessibility:** Faster models can run on cheaper hardware, opening up AI to underserved regions or small businesses.\n",
            "* **Regulatory & privacy compliance:** Local inference keeps data on device, satisfying privacy regulations that forbid cloud uploads.\n",
            "\n",
            "---\n",
            "\n",
            "## 6. The Cost of Slowness\n",
            "\n",
            "1. **Compute Costs:** Inference is billed per second of GPU usage. A 10× slower model means 10× the bill.\n",
            "2. **User Drop‑off:** Every 100 ms increase in latency can reduce conversion by ~5 % in e‑commerce sites.\n",
            "3. **Carbon Footprint:** More GPU time = more energy consumed. Faster inference translates to fewer watts per inference.\n",
            "\n",
            "---\n",
            "\n",
            "## 7. Case Studies\n",
            "\n",
            "| Company | Challenge | Solution | Result |\n",
            "|---------|-----------|----------|--------|\n",
            "| **OpenAI** | Deploy GPT‑3 for interactive chat | Optimized 4‑bit quantization, model pruning, and efficient beam search | 3× lower latency, 60 % cost savings |\n",
            "| **Google** | On‑device keyboard suggestions | Distilled BERT‑like model, 8‑bit quantization | 10 ms inference on Pixel phone |\n",
            "| **Meta** | Real‑time dialogue in VR | Sparse transformers + streaming inference | Maintained < 100 ms round‑trip on modest GPU |\n",
            "| **Alibaba** | Search ranking at 10 M QPS | Pipeline parallelism, custom hardware kernels | 5× throughput, 70 % reduction in GPU count |\n",
            "\n",
            "---\n",
            "\n",
            "## 8. Future Directions\n",
            "\n",
            "1. **Adaptive Models**: Switch between a tiny fast model and a larger one based on request complexity.\n",
            "2. **Neural Architecture Search (NAS) for latency**: Automate design of models that hit specific latency budgets.\n",
            "3. **Hardware‑in‑the‑loop**: Design models in tandem with custom ASICs or FPGAs for maximum efficiency.\n",
            "4. **Energy‑aware training**: Incorporate latency and power as training objectives.\n",
            "\n",
            "---\n",
            "\n",
            "### Bottom Line\n",
            "\n",
            "Fast language models are not merely a convenience—they’re the **enabler** of scalable, cost‑effective, and user‑friendly AI systems across industries. Whether you’re building a chatbot that needs to answer in 200 ms, running inference on a battery‑powered device, or scaling to millions of requests per second in the cloud, speed is the metric that turns a good model into a great one. Investing in techniques that reduce latency while preserving quality is now as critical as improving raw performance metrics.\n"
          ]
        }
      ],
      "id": "LSjZyfixrvvo"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iacPmN3X1iry",
        "outputId": "b52fede6-74e0-4f88-f6a8-a474ef043c07"
      },
      "id": "iacPmN3X1iry",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Groq\n",
            "  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from Groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from Groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from Groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from Groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from Groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->Groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->Groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->Groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->Groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->Groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->Groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->Groq) (0.4.1)\n",
            "Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Groq\n",
            "Successfully installed Groq-0.31.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "os.environ['GROQ_API_KEY'] = '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'"
      ],
      "metadata": {
        "id": "UIwGMBVIz8VV"
      },
      "id": "UIwGMBVIz8VV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5oX_c5Nrvvp",
        "outputId": "38287a82-6d84-4da8-88a7-7268e4bd3899"
      },
      "source": [
        "# Demo: parse 3 sample chats and validate against the simple schema\n",
        "samples = [\n",
        "    \"Hi, my name is Alice Johnson. I'm 29 years old and I'm from Bangalore. You can reach me at alice.j@example.com or +91 98765-43210.\",\n",
        "    \"Hello, this is Bob. bob_smith@mail.com. Age: 42. Lives in San Francisco. Phone: +1 415 555 2671\",\n",
        "    \"Hey, I am Carlos. I'm 25yo and live in Madrid. Email: carlos(at)example(dot)com (please fix), phone 666777888\"\n",
        "]\n",
        "for i, s in enumerate(samples, 1):\n",
        "    parsed = simulate_function_call_parse(s)\n",
        "    ok, errors = simple_validate(parsed, SCHEMA)\n",
        "    print(f'--- Sample {i} ---')\n",
        "    print('Chat text:', s)\n",
        "    print('Parsed:', json.dumps(parsed, indent=2))\n",
        "    print('Valid:', ok)\n",
        "    if not ok:\n",
        "        print('Errors:', errors)\n",
        "    print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sample 1 ---\n",
            "Chat text: Hi, my name is Alice Johnson. I'm 29 years old and I'm from Bangalore. You can reach me at alice.j@example.com or +91 98765-43210.\n",
            "Parsed: {\n",
            "  \"name\": \"Alice Johnson\",\n",
            "  \"email\": \"alice.j@example.com\",\n",
            "  \"location\": \"Bangalore\",\n",
            "  \"age\": 29\n",
            "}\n",
            "Valid: True\n",
            "\n",
            "\n",
            "--- Sample 2 ---\n",
            "Chat text: Hello, this is Bob. bob_smith@mail.com. Age: 42. Lives in San Francisco. Phone: +1 415 555 2671\n",
            "Parsed: {\n",
            "  \"name\": \"Bob\",\n",
            "  \"email\": \"bob_smith@mail.com\",\n",
            "  \"location\": \"San Francisco\"\n",
            "}\n",
            "Valid: True\n",
            "\n",
            "\n",
            "--- Sample 3 ---\n",
            "Chat text: Hey, I am Carlos. I'm 25yo and live in Madrid. Email: carlos(at)example(dot)com (please fix), phone 666777888\n",
            "Parsed: {\n",
            "  \"name\": \"Carlos\",\n",
            "  \"location\": \"Madrid\",\n",
            "  \"age\": 25\n",
            "}\n",
            "Valid: True\n",
            "\n",
            "\n"
          ]
        }
      ],
      "id": "h5oX_c5Nrvvp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p4iyjb9rvvp"
      },
      "source": [],
      "id": "8p4iyjb9rvvp"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Q7HIiZC_EQ8"
      },
      "id": "3Q7HIiZC_EQ8",
      "execution_count": null,
      "outputs": []
    }
  ]
}